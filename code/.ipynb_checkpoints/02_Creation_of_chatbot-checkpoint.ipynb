{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97561240",
   "metadata": {},
   "source": [
    "## Part 3: Creating a customised chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9cbe72",
   "metadata": {},
   "source": [
    "Based on part 1 and 2 in the earlier notebooks, I have extracted the necessary data that I need as context for building a customised chatbot. \n",
    "The following portion of this codebook will include:\n",
    "- Part 3a: Creating a default chatbot without introducing any system prompt\n",
    "- Part 3b: Creating an improved chatbot with the use of system prompt\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "365d3559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from llama_index import Document, GPTVectorStoreIndex, ServiceContext\n",
    "from llama_index.readers import SimpleDirectoryReader\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50345cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will need to use our own OpenAI API key. This key is removed due to privacy issue.\n",
    "os.environ['OPENAI_API_KEY'] = \"< Include your own OpenAI API key>\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b38c4902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 125 docs\n"
     ]
    }
   ],
   "source": [
    "filename_fn = lambda filename: {'file_name': filename}\n",
    "my_docs = SimpleDirectoryReader(input_dir=\"../data\", exclude_hidden=True, file_metadata=filename_fn).load_data()\n",
    "\n",
    "print(f\"Loaded {len(my_docs)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b80b4",
   "metadata": {},
   "source": [
    "### Part 3a: Creating a default chatbot without any system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e438825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the original without any prompts for the chatbot\n",
    "original_service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd3edae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_index = GPTVectorStoreIndex.from_documents(documents=my_docs, service_context=original_service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29fd12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_query_engine = original_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61359510",
   "metadata": {},
   "source": [
    "#### Testing out some questions with the original query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7c78d5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A candidate must earn a fixed monthly salary starting from $5,000 to be eligible for an employment pass. However, candidates in the financial services sector need to earn higher salaries to qualify.\n",
      "\n",
      "This query took: 8.021786212921143 secs.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "response = original_query_engine.query(\"How much salary must a candidate earn to be eligible for employment pass?\")\n",
    "print(response)\n",
    "end = time.time()\n",
    "print(\"\")\n",
    "print(f\"This query took: {end-start} secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bbc9a4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can earn 20 points under the salary criteria by having a fixed monthly salary that is at or above the 90th percentile compared to the salary benchmarks by sector.\n",
      "\n",
      "This query took: 8.367677211761475 secs.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "response = original_query_engine.query(\"How do I earn 20 points under the salary criteria?\")\n",
    "print(response)\n",
    "end = time.time()\n",
    "print(\"\")\n",
    "print(f\"This query took: {end-start} secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "80ecb540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can find information about your company's diversity in the \"Diversity\" tab of the Workforce Insights tool, which can be accessed via the myMOM Portal.\n",
      "\n",
      "This query took: 8.531136989593506 secs.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "response = original_query_engine.query(\"Where can I find information about my company's diversity?\")\n",
    "print(response)\n",
    "end = time.time()\n",
    "print(\"\")\n",
    "print(f\"This query took: {end-start} secs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2a772",
   "metadata": {},
   "source": [
    "**Based on the above three queries that we have tested, the chatbot is able to come up with succint answers from the queries we have asked about qualifications, salary and diversity. But, we would ideally hope that the chatbot is able to include more context to how it derived the answer.** \n",
    "\n",
    "**As for the runtime, each query took on average about 8.3 secs to complete, which is pretty good based on a study conducted by HubSpot (screenshot below) that found the average response time across industries for chatbots to be 9.3 seconds.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da245731",
   "metadata": {},
   "source": [
    "![Average response time](../images/Average_response_time_of_chatbot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e0a03",
   "metadata": {},
   "source": [
    "#### Generating the questions to measure the performance of the original query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ae8aed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the documents\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(my_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b277c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen_query = (\n",
    "    \"You are working at Ministry of Manpower focusing on eligibiity requirements of the employment pass. \\\n",
    "    There is a new complementarity assessment framework that will assess the eligibility of all prospective employment pass holders. \\\n",
    "    Your task is to setup all possible questions and requests, \\\n",
    "    using the provided context from documents on eligibility of employment pass, \\\n",
    "    formulate questions that capture important facts from the context. \\\n",
    "    Restrict the question to the context information provided.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8c92eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    my_docs,\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=original_service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f7934eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  30  questions\n"
     ]
    }
   ],
   "source": [
    "questions = dataset_generator.generate_questions_from_nodes(num=30)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bafffab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../qns_and_eval/original_evaluation_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c896e1",
   "metadata": {},
   "source": [
    "#### Generating the evaluation metrics to find out the performance of the original query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "529febe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_contexts = []\n",
    "original_answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = original_query_engine.query(question)\n",
    "    original_contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    original_answers.append(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4f5f4c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████                       | 1/2 [01:58<01:58, 118.37s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Wed, 08 Nov 2023 09:41:14 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '822cda7e7dee46f7-SIN', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "100%|██████████████████████████████████████████████| 2/2 [04:42<00:00, 141.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                       | 0/2 [00:00<?, ?it/s]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "100%|█████████████████████████████████████████████| 2/2 [34:36<00:00, 1038.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.8767, 'answer_relevancy': 0.9227, 'faithfulness': 0.8350}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "original_ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": original_answers,\n",
    "        \"contexts\": original_contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "original_result = evaluate(original_ds, [answer_relevancy, faithfulness])\n",
    "print(original_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2fab5",
   "metadata": {},
   "source": [
    "**Based on the evaluation metrics, the original query engine seems to be performing decent enough to be deployed on streamlit. All three metrics are quite close to 1, which indicates a good performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb4e02",
   "metadata": {},
   "source": [
    "### Part 3b: Improved chatbot with system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a75f42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the improved service context with context_window and system prompt added for the chatbot\n",
    "improved_service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0), \n",
    "    context_window=2048, \n",
    "    system_prompt = \"You are an expert who understands the eligibility criteria of employment pass and your job is to answer questions related to the COMPASS and all relevant requirements. Keep your answers factual and provide more context. When asked about salary criteria or C1, include both the age and sector assumed if not provided before answering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "62353029",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_index = GPTVectorStoreIndex.from_documents(documents=my_docs, service_context=improved_service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ee155726",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_query_engine = improved_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a28fe81",
   "metadata": {},
   "source": [
    "#### Testing out some questions with the improved query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "be6b1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A candidate must have a fixed monthly salary starting from $5,000 to be eligible for an Employment Pass. The salary increases progressively with age, up to $10,500 for those in the mid-40s. However, candidates in the financial services sector must earn at least $5,500, with the salary also increasing progressively with age up to $11,500 for those in the mid-40s.\n",
      "\n",
      "This query took: 18.43025517463684 secs.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "response = improved_query_engine.query(\"How much salary must a candidate earn to be eligible for employment pass?\")\n",
    "print(response)\n",
    "end = time.time()\n",
    "print(\"\")\n",
    "print(f\"This query took: {end-start} secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c960c35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To earn 20 points under the salary criteria, your candidate's fixed monthly salary should be at or above the 90th percentile of the salary benchmarks by sector.\n",
      "\n",
      "This query took: 8.806796073913574 secs.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "response = improved_query_engine.query(\"How do I earn 20 points under the salary criteria?\")\n",
    "print(response)\n",
    "end = time.time()\n",
    "print(\"\")\n",
    "print(f\"This query took: {end-start} secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "20c3a66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can find information about your company's diversity in the \"Diversity\" tab of the Workforce Insights tool on the myMOM Portal. This tab shows you the top nationalities of PMETs (Professionals, Managers, Executives, and Technicians) in your firm, allowing you to assess the diversity of your workforce.\n",
      "\n",
      "This query took: 6.932214736938477 secs.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "response = improved_query_engine.query(\"Where can I find information about my company's diversity?\")\n",
    "print(response)\n",
    "end = time.time()\n",
    "print(\"\")\n",
    "print(f\"This query took: {end-start} secs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0356b",
   "metadata": {},
   "source": [
    "**Based on the above three queries that we have tested, the chatbot is able to come up with a more comprehensive answers from the queries we have asked about qualifications, salary and diversity. But, we would ideally hope that the chatbot is able to include more context to how it derived the answer.** \n",
    "\n",
    "**As for the runtime, it took on average 11.3 secs, much longer than the earlier original query engine. While it may take longer than the average response time from the study by HubSpot, we can see that given the same set of questions, the improved chatbot is able to provide more context, which is more suitable for our use case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563ff90",
   "metadata": {},
   "source": [
    "#### Generating the questions to measure the performance of the improved query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8e659e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the documents\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(my_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "17c7e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen_query = (\n",
    "    \"You are working at Ministry of Manpower focusing on eligibiity requirements of the employment pass. \\\n",
    "    There is a new complementarity assessment framework that will assess the eligibility of all prospective employment pass holders. \\\n",
    "    Your task is to setup all possible questions and requests, \\\n",
    "    using the provided context from documents on eligibility of employment pass, \\\n",
    "    formulate questions that capture important facts from the context. \\\n",
    "    Restrict the question to the context information provided.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "42c8de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_dataset_generator = DatasetGenerator.from_documents(\n",
    "    my_docs,\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=improved_service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "06446b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  30  questions\n"
     ]
    }
   ],
   "source": [
    "improved_questions = improved_dataset_generator.generate_questions_from_nodes(num=30)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b953b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../qns_and_eval/improved_evaluation_questions.txt\", \"w\") as f:\n",
    "    for question in improved_questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d15329",
   "metadata": {},
   "source": [
    "#### Generating the evaluation metrics to find out the performance of the improved query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4885d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_eval_contexts = []\n",
    "improved_eval_answers = []\n",
    "\n",
    "for question in improved_questions:\n",
    "    improved_eval_response = improved_query_engine.query(question)\n",
    "    improved_eval_contexts.append([x.node.get_content() for x in improved_eval_response.source_nodes])\n",
    "    improved_eval_answers.append(str(improved_eval_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "838b5df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 2/2 [03:05<00:00, 92.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                       | 0/2 [00:00<?, ?it/s]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      " 50%|██████████████████████▌                      | 1/2 [27:56<27:56, 1676.33s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "100%|█████████████████████████████████████████████| 2/2 [48:17<00:00, 1448.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.8310, 'answer_relevancy': 0.9606, 'faithfulness': 0.7322}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "improved_eval_ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": improved_questions,\n",
    "        \"answer\": improved_eval_answers,\n",
    "        \"contexts\": improved_eval_contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "improved_eval_result = evaluate(improved_eval_ds, [answer_relevancy, faithfulness])\n",
    "print(improved_eval_result) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1318c5f",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939ef75",
   "metadata": {},
   "source": [
    "**The summary of the  evaluation metrics and average response time of both chatbots of slightly different configured query engines can be found in the table below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aeab66",
   "metadata": {},
   "source": [
    "| Query engine         | RAGAS Score | Answer Relevancy | Faithfulness | Average response time |\n",
    "|----------------------|-------------|------------------|--------------|-----------------------|\n",
    "| Original (GPT-3.5-Turbo) | 0.8767 | 0.9227        | 0.8350       | 8.3 secs |\n",
    "| Improved with system prompt (GPT-3.5-Turbo) | 0.8310| 0.9606      | 0.7322 | 11.3 secs |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaba3ff",
   "metadata": {},
   "source": [
    "**At a glance, we can see that the Original (GPT-3.5-Turbo) seems to be better in most attributes, aside from answer relevancy.** \n",
    "\n",
    "**However, for our use case, we are focusing slightly more on the answer relevancy. This is because we would like to focus more on the appropriateness and completeness of the responses to the questions posted by the user. There is however, room for better improvement to improve the faithfulness as well as the overall RAGAS score as a lower faithfulness score would imply a lower factual consistency in the responses.**\n",
    "\n",
    "**We will use the improved query engine with system prompt for our chatbot demonstration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "68ee0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We store the vectors of all the documents that we have curated for our chatbot that is to be deployed on streamlit\n",
    "improved_index.storage_context.persist(persist_dir=\"../streamlit/improved_index.vecstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0526d387",
   "metadata": {},
   "source": [
    "With the vectors of the documents stored, we are now ready to use it for our chatbot on streamlit (https://chatbot-compass.streamlit.app/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
